{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessory packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score,make_scorer\n",
    "from nltk.corpus import brown\n",
    "from statistics import stdev, mean\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for loading brown corpus and assigning labels as fiction and non-fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_brown_corpus():\n",
    "    \"\"\"Load and process Brown corpus data\"\"\"\n",
    "    fiction_categories = ['fiction', 'mystery', 'romance', 'adventure', 'science_fiction']\n",
    "    nonfiction_categories = ['news', 'reviews', 'hobbies', 'government', 'learned']\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for category in brown.categories():\n",
    "        if category in fiction_categories:\n",
    "            for fileid in brown.fileids(category):\n",
    "                words = brown.words(fileid)\n",
    "                texts.append(' '.join(words))\n",
    "                labels.append(1) # fiction as 1\n",
    "        elif category in nonfiction_categories:\n",
    "            for fileid in brown.fileids(category):\n",
    "                words = brown.words(fileid)\n",
    "                texts.append(' '.join(words))\n",
    "                labels.append(0) #non-fiction as 0\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting most significat features (only adverb_adjective ratio and adjective_pronoun ratio )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_two_features(text):\n",
    "    \"\"\"Extract POS tag ratios (only two most significant features)\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    adverb_count = sum(1 for _, tag in pos_tags if tag.startswith('RB'))\n",
    "    adjective_count = sum(1 for _, tag in pos_tags if tag.startswith('JJ'))\n",
    "    pronoun_count = sum(1 for _, tag in pos_tags if tag.startswith(('PRP', 'WP')))\n",
    "    \n",
    "    if adjective_count>0:\n",
    "        adv_adj_ratio = adverb_count / adjective_count \n",
    "    else:\n",
    "        adv_adj_ratio=0\n",
    "    if pronoun_count>0:\n",
    "        adj_pro_ratio = adjective_count / pronoun_count \n",
    "    else:\n",
    "        adj_pro_ratio=0\n",
    "    \n",
    "    return [adv_adj_ratio, adj_pro_ratio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Low level features as mentioned in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_low_level_features(text):\n",
    "    \"\"\"Extract all low-level features\"\"\"\n",
    "    # Tokenization\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Basic calculations\n",
    "    sentence_lengths = [len(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "    word_lengths = [len(word) for word in words if word.isalnum()]\n",
    "    \n",
    "    # Token/type calculations\n",
    "    tokens = [word.lower() for word in words]\n",
    "    types = set(tokens)\n",
    "    token_type_ratio = len(tokens) / len(types) if types else 0\n",
    "    \n",
    "    # Punctuation calculations\n",
    "    punct_tokens = [word for word in words if not word.isalnum()]\n",
    "    punct_types = set(punct_tokens)\n",
    "    punct_ratio = len(punct_tokens) / len(punct_types) if punct_types else 0\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punct_counts = Counter(punct_tokens)\n",
    "    hyphen_count = punct_counts['-']\n",
    "    quote_count = punct_counts['\"'] + punct_counts[\"'\"]\n",
    "    exclaim_count = punct_counts['!']\n",
    "    question_count = punct_counts['?']\n",
    "    \n",
    "    return [\n",
    "        mean(sentence_lengths) if sentence_lengths else 0,  # Avg sentence length\n",
    "        mean(word_lengths) if word_lengths else 0,  # Avg word length\n",
    "        stdev(sentence_lengths) if len(sentence_lengths) > 1 else 0,  # StdDev sentence length\n",
    "        stdev(word_lengths) if len(word_lengths) > 1 else 0,  # StdDev word length\n",
    "        token_type_ratio,  # Avg token/type\n",
    "        0,  # StdDev token/type (calculated over windows in practice)\n",
    "        punct_ratio,  # Avg token/type (punctuation)\n",
    "        0,  # StdDev token/type punctuation (calculated over windows in practice)\n",
    "        hyphen_count / (quote_count + 1),  # Hyphen/Quote\n",
    "        hyphen_count / (exclaim_count + 1),  # Hyphen/Exclamation\n",
    "        quote_count / (question_count + 1)  # Quote/Question\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting pos-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_features(text):\n",
    "    \"\"\"Extract POS-based features\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Count POS tags\n",
    "    adverb_count = sum(1 for _, tag in pos_tags if tag.startswith('RB'))\n",
    "    adjective_count = sum(1 for _, tag in pos_tags if tag.startswith('JJ'))\n",
    "    noun_count = sum(1 for _, tag in pos_tags if tag.startswith('NN'))\n",
    "    verb_count = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
    "    pronoun_count = sum(1 for _, tag in pos_tags if tag.startswith(('PRP', 'WP')))\n",
    "    \n",
    "    # Calculate ratios (adding 1 to denominators to avoid division by zero)\n",
    "    features = [\n",
    "        adverb_count / (noun_count + 1),      # Adverb/Noun\n",
    "        adverb_count / (pronoun_count + 1),    # Adverb/Pronoun\n",
    "        adjective_count / (verb_count + 1),    # Adjective/Verb\n",
    "        noun_count / (verb_count + 1),         # Noun/Verb\n",
    "        verb_count / (pronoun_count + 1),      # Verb/Pronoun\n",
    "        adverb_count / (adjective_count + 1),  # Adverb/Adjective\n",
    "        adjective_count / (pronoun_count + 1), # Adjective/Pronoun\n",
    "        noun_count / (pronoun_count + 1)       # Noun/Pronoun\n",
    "    ]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting 6 Features as mentioned in the paper \n",
    "- Avg sentence length, \n",
    "- Avg word length, \n",
    "- Hyphen/Quote, \n",
    "- Adverb/Adjective,  \n",
    "- Adjective/Pronoun, \n",
    "- Noun/Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_six_features(text):\n",
    "    \"\"\"Extract the 6 specified features\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Basic features\n",
    "    sentence_lengths = [len(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "    word_lengths = [len(word) for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Punctuation counts\n",
    "    punct_counts = Counter(token for token in tokens if not token.isalnum())\n",
    "    hyphen_count = punct_counts['-']\n",
    "    quote_count = punct_counts['\"'] + punct_counts[\"'\"]\n",
    "    \n",
    "    # POS counts\n",
    "    adverb_count = sum(1 for _, tag in pos_tags if tag.startswith('RB'))\n",
    "    adjective_count = sum(1 for _, tag in pos_tags if tag.startswith('JJ'))\n",
    "    noun_count = sum(1 for _, tag in pos_tags if tag.startswith('NN'))\n",
    "    pronoun_count = sum(1 for _, tag in pos_tags if tag.startswith(('PRP', 'WP')))\n",
    "    \n",
    "    return [\n",
    "        mean(sentence_lengths) if sentence_lengths else 0,  # Avg sentence length\n",
    "        mean(word_lengths) if word_lengths else 0,  # Avg word length\n",
    "        hyphen_count / (quote_count + 1),  # Hyphen/Quote\n",
    "        adverb_count / (adjective_count + 1),  # Adverb/Adjective\n",
    "        adjective_count / (pronoun_count + 1),  # Adjective/Pronoun\n",
    "        noun_count / (pronoun_count + 1)  # Noun/Pronoun\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selecting which type of fetures to be extracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_by_type(text, feature_type='all'):\n",
    "    \"\"\"Extract features based on specified type\"\"\"\n",
    "    if feature_type == 'two':\n",
    "        return extract_two_features(text)\n",
    "    elif feature_type == 'low_level':\n",
    "        return extract_low_level_features(text)\n",
    "    elif feature_type == 'nineteen':\n",
    "        return extract_low_level_features(text) + extract_pos_features(text)\n",
    "    elif feature_type == 'six':\n",
    "        return extract_six_features(text)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature type. Choose 'two', 'low_level', 'nineteen', or 'six'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on given datasets as arguments and giving performance matrices after ten-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X, y):\n",
    "    \"\"\"Run experiment with cross-validation using Logistic Regression\"\"\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "    \n",
    "    # Create stratified 10-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Custom scoring functions\n",
    "    f1_nonfiction_scorer = make_scorer(f1_score, pos_label=0)\n",
    "    f1_fiction_scorer = make_scorer(f1_score, pos_label=1)\n",
    "    # Perform cross-validation for each metric\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    f1_nonfiction_scores = cross_val_score(model, X, y, cv=cv, scoring=f1_nonfiction_scorer)\n",
    "    f1_fiction_scores = cross_val_score(model, X, y, cv=cv, scoring=f1_fiction_scorer)\n",
    "    \n",
    "    # Calculate baseline accuracy for each fold\n",
    "    baseline_accuracies = []\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        y_test = y[test_idx]\n",
    "        baseline = max(np.sum(y_test == 0) / len(y_test),\n",
    "                    np.sum(y_test == 1) / len(y_test))\n",
    "        baseline_accuracies.append(baseline)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': np.mean(accuracy_scores),\n",
    "        'accuracy_std': np.std(accuracy_scores),\n",
    "        'f1_nonfiction': np.mean(f1_nonfiction_scores),\n",
    "        'f1_nonfiction_std': np.std(f1_nonfiction_scores),\n",
    "        'f1_fiction': np.mean(f1_fiction_scores),\n",
    "        'f1_fiction_std': np.std(f1_fiction_scores),\n",
    "        'baseline': np.mean(baseline_accuracies),\n",
    "        'baseline_std': np.std(baseline_accuracies)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviding datasets similar to given in paper and feeding to the model, storing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments_for_feature_set(brown_texts, brown_labels, bnc_texts, bnc_labels, feature_type):\n",
    "    \"\"\"Run all experiments for a given feature set\"\"\"\n",
    "    print(f\"\\nExtracting {feature_type} features...\")\n",
    "    brown_features = np.array([extract_features_by_type(text, feature_type) for text in brown_texts])\n",
    "    bnc_features = np.array([extract_features_by_type(text, feature_type) for text in bnc_texts])\n",
    "    \n",
    "    # Run experiments\n",
    "    print(f\"Running experiments for {feature_type} features...\")\n",
    "    \n",
    "    # 1. Brown Corpus only\n",
    "    results_brown = run_experiment(brown_features, brown_labels)\n",
    "    \n",
    "    # 2. Brown + BNC combined\n",
    "    combined_features = np.vstack((brown_features, bnc_features))\n",
    "    combined_labels = np.concatenate((brown_labels, bnc_labels))\n",
    "    results_combined = run_experiment(combined_features, combined_labels)\n",
    "    \n",
    "    # 3. Train on Brown, test on BNC\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(brown_features, brown_labels)\n",
    "    bnc_pred = model.predict(bnc_features)\n",
    "    results_bnc = {\n",
    "        'accuracy': accuracy_score(bnc_labels, bnc_pred),\n",
    "        'f1_nonfiction': f1_score(bnc_labels, bnc_pred, pos_label=0),\n",
    "        'f1_fiction': f1_score(bnc_labels, bnc_pred, pos_label=1),\n",
    "        'baseline': len(bnc_labels[bnc_labels == 0])/len(bnc_labels)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'brown': results_brown,\n",
    "        'combined': results_combined,\n",
    "        'bnc': results_bnc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(all_results):\n",
    "    \"\"\"Print results for all feature types and experiments in a organized way\"\"\"\n",
    "    feature_names = {\n",
    "        'two': 'Original Two Features (Adverb/Adjective, Adjective/Pronoun)',\n",
    "        'low_level': 'All Low Level Features (11 features)',\n",
    "        'nineteen': '19 Features (Combined)',\n",
    "        'six': '6 Selected Features'\n",
    "    }\n",
    "    \n",
    "    for feature_type, results in all_results.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Results for {feature_names[feature_type]}:\")\n",
    "        print('='*80)\n",
    "        \n",
    "        print(\"\\nBrown Corpus (60-40 split):\")\n",
    "        print(f\"Accuracy: {100*results['brown']['accuracy']:.2f} ± {results['brown']['accuracy_std']:.2f}\")\n",
    "        print(f\"F1 (non-fiction): {results['brown']['f1_nonfiction']:.4f} ± {results['brown']['f1_nonfiction_std']:.4f}\")\n",
    "        print(f\"F1 (fiction): {results['brown']['f1_fiction']:.4f} ± {results['brown']['f1_fiction_std']:.4f}\")\n",
    "        print(f\"Baseline: {100*results['brown']['baseline']:.2f} ± {results['brown']['baseline_std']:.2f}\")\n",
    "        \n",
    "        print(\"\\nBrown + BNC combined:\")\n",
    "        print(f\"Accuracy: {100*results['combined']['accuracy']:.2f} ± {results['combined']['accuracy_std']:.2f}\")\n",
    "        print(f\"F1 (non-fiction): {results['combined']['f1_nonfiction']:.4f} ± {results['combined']['f1_nonfiction_std']:.4f}\")\n",
    "        print(f\"F1 (fiction): {results['combined']['f1_fiction']:.4f} ± {results['combined']['f1_fiction_std']:.4f}\")\n",
    "        print(f\"Baseline: {100*results['combined']['baseline']:.2f} ± {results['combined']['baseline_std']:.2f}\")\n",
    "        \n",
    "        print(\"\\nTrain on Brown, Test on BNC:\")\n",
    "        print(f\"Accuracy: {100*results['bnc']['accuracy']:.2f}\")\n",
    "        print(f\"F1 (non-fiction): {results['bnc']['f1_nonfiction']:.4f}\")\n",
    "        print(f\"F1 (fiction): {results['bnc']['f1_fiction']:.4f}\")\n",
    "        print(f\"Baseline: {100*results['bnc']['baseline']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Extracting two features...\n",
      "Running experiments for two features...\n",
      "\n",
      "Extracting low_level features...\n",
      "Running experiments for low_level features...\n",
      "\n",
      "Extracting nineteen features...\n",
      "Running experiments for nineteen features...\n",
      "\n",
      "Extracting six features...\n",
      "Running experiments for six features...\n",
      "\n",
      "================================================================================\n",
      "Results for Original Two Features (Adverb/Adjective, Adjective/Pronoun):\n",
      "================================================================================\n",
      "\n",
      "Brown Corpus (60-40 split):\n",
      "Accuracy: 97.51 ± 0.03\n",
      "F1 (non-fiction): 0.9801 ± 0.0221\n",
      "F1 (fiction): 0.9665 ± 0.0357\n",
      "Baseline: 63.89 ± 0.01\n",
      "\n",
      "Brown + BNC combined:\n",
      "Accuracy: 97.89 ± 0.02\n",
      "F1 (non-fiction): 0.9830 ± 0.0185\n",
      "F1 (fiction): 0.9722 ± 0.0304\n",
      "Baseline: 62.53 ± 0.01\n",
      "\n",
      "Train on Brown, Test on BNC:\n",
      "Accuracy: 100.00\n",
      "F1 (non-fiction): 1.0000\n",
      "F1 (fiction): 1.0000\n",
      "Baseline: 54.55\n",
      "\n",
      "================================================================================\n",
      "Results for All Low Level Features (11 features):\n",
      "================================================================================\n",
      "\n",
      "Brown Corpus (60-40 split):\n",
      "Accuracy: 94.46 ± 0.04\n",
      "F1 (non-fiction): 0.9562 ± 0.0324\n",
      "F1 (fiction): 0.9244 ± 0.0536\n",
      "Baseline: 63.89 ± 0.01\n",
      "\n",
      "Brown + BNC combined:\n",
      "Accuracy: 95.25 ± 0.03\n",
      "F1 (non-fiction): 0.9620 ± 0.0287\n",
      "F1 (fiction): 0.9360 ± 0.0460\n",
      "Baseline: 62.53 ± 0.01\n",
      "\n",
      "Train on Brown, Test on BNC:\n",
      "Accuracy: 54.55\n",
      "F1 (non-fiction): 0.7059\n",
      "F1 (fiction): 0.0000\n",
      "Baseline: 54.55\n",
      "\n",
      "================================================================================\n",
      "Results for 19 Features (Combined):\n",
      "================================================================================\n",
      "\n",
      "Brown Corpus (60-40 split):\n",
      "Accuracy: 97.22 ± 0.02\n",
      "F1 (non-fiction): 0.9780 ± 0.0175\n",
      "F1 (fiction): 0.9618 ± 0.0296\n",
      "Baseline: 63.89 ± 0.01\n",
      "\n",
      "Brown + BNC combined:\n",
      "Accuracy: 97.11 ± 0.03\n",
      "F1 (non-fiction): 0.9768 ± 0.0218\n",
      "F1 (fiction): 0.9615 ± 0.0373\n",
      "Baseline: 62.53 ± 0.01\n",
      "\n",
      "Train on Brown, Test on BNC:\n",
      "Accuracy: 100.00\n",
      "F1 (non-fiction): 1.0000\n",
      "F1 (fiction): 1.0000\n",
      "Baseline: 54.55\n",
      "\n",
      "================================================================================\n",
      "Results for 6 Selected Features:\n",
      "================================================================================\n",
      "\n",
      "Brown Corpus (60-40 split):\n",
      "Accuracy: 97.52 ± 0.02\n",
      "F1 (non-fiction): 0.9805 ± 0.0149\n",
      "F1 (fiction): 0.9659 ± 0.0254\n",
      "Baseline: 63.89 ± 0.01\n",
      "\n",
      "Brown + BNC combined:\n",
      "Accuracy: 97.37 ± 0.03\n",
      "F1 (non-fiction): 0.9783 ± 0.0261\n",
      "F1 (fiction): 0.9664 ± 0.0388\n",
      "Baseline: 62.53 ± 0.01\n",
      "\n",
      "Train on Brown, Test on BNC:\n",
      "Accuracy: 100.00\n",
      "F1 (non-fiction): 1.0000\n",
      "F1 (fiction): 1.0000\n",
      "Baseline: 54.55\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "brown_texts, brown_labels = load_brown_corpus()\n",
    "\n",
    "df=pd.read_csv(\"baby_bnc.csv\") #pre processed labeled csv file\n",
    "bnc_texts=df[\"text\"].tolist()\n",
    "df['label'] = df['label'].map({'fiction': 1, 'non-fiction': 0}) #our csv has labels as fiction or non-fiction\n",
    "bnc_labels = df['label'].tolist()\n",
    "\n",
    "brown_labels = np.array(brown_labels)\n",
    "bnc_labels = np.array(bnc_labels)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "feature_types = ['two', 'low_level', 'nineteen', 'six']\n",
    "\n",
    "for feature_type in feature_types:\n",
    "    all_results[feature_type] = run_experiments_for_feature_set(\n",
    "        brown_texts, brown_labels, bnc_texts, bnc_labels, feature_type\n",
    "    )\n",
    "\n",
    "# Print all results in an organized way\n",
    "print_results(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
